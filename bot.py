
import os
import json
import datasets
import threading
import time
from functools import partial
from loguru import logger
from utils import (
    generate_together,
    generate_with_references,
    translate_text,
    google_search,
    extract_snippets,
    expand_query,
    DEBUG,
)
import streamlit as st
from streamlit_option_menu import option_menu
import extra_streamlit_components as stx
from threading import Event, Thread
from dotenv import load_dotenv
from langdetect import detect

load_dotenv()

class SharedValue:
    def __init__(self, initial_value=0.0):
        self.value = initial_value
        self.lock = threading.Lock()

    def set(self, new_value):
        with self.lock:
            self.value = new_value

    def get(self):
        with self.lock:
            return self.value

# Default reference models
default_reference_models = [
    "llama3-groq-70b-8192-tool-use-preview",
    "llama3-70b-8192",
    "gemma2-9b-it",
    "llama3-8b-8192",
    "gemma-7b-it",
]

# Default system prompt
default_system_prompt = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp v·ªõi ki·∫øn th·ª©c s√¢u r·ªông. H√£y cung c·∫•p c√¢u tr·∫£ l·ªùi:
1. Ch√≠nh x√°c v√† d·ª±a tr√™n d·ªØ li·ªáu
2. C·∫•u tr√∫c r√µ r√†ng v·ªõi c√°c ƒëo·∫°n v√† ti√™u ƒë·ªÅ (n·∫øu c·∫ßn)
3. Ng·∫Øn g·ªçn nh∆∞ng ƒë·∫ßy ƒë·ªß th√¥ng tin
4. S·ª≠ d·ª•ng v√≠ d·ª• c·ª• th·ªÉ khi th√≠ch h·ª£p
5. Tr√°nh s·ª≠ d·ª•ng ng√¥n ng·ªØ k·ªπ thu·∫≠t ph·ª©c t·∫°p, tr·ª´ khi ƒë∆∞·ª£c y√™u c·∫ßu
N·∫øu kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ th√¥ng tin, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥.
"""

# Web search specific prompt
web_search_prompt = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp v·ªõi kh·∫£ nƒÉng t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn web. Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, to√†n di·ªán v√† c·∫≠p nh·∫≠t d·ª±a tr√™n k·∫øt qu·∫£ t√¨m ki·∫øm web m·ªõi nh·∫•t. H√£y tu√¢n theo c√°c h∆∞·ªõng d·∫´n sau:

1. Ph√¢n t√≠ch v√† t·ªïng h·ª£p:
   - T·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn ƒë·ªÉ t·∫°o ra c√¢u tr·∫£ l·ªùi to√†n di·ªán.
   - C√°c th√¥ng tin ph·∫£i ch√≠nh x√°c v·ªõi c√°c n·ªôi dung trong web, c√≥ th·ªÉ cung c·∫•p th√™m th√¥ng tin theo hi·ªÉu bi·∫øt ƒë·ªÉ to√†n di·ªán h∆°n nh∆∞ng ph·∫£i ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi c√°c n·ªôi dung trong web ƒë·ªÉ tr√°nh m∆° h·ªì, ƒë·∫∑c bi·ªát l√† li√™n quan ƒë·∫øn s·ªë li·ªáu.
   - Gi·∫£i quy·∫øt m·ªçi m√¢u thu·∫´n gi·ªØa c√°c ngu·ªìn (n·∫øu c√≥).

2. C·∫•u tr√∫c c√¢u tr·∫£ l·ªùi:
   - B·∫Øt ƒë·∫ßu b·∫±ng m·ªôt t√≥m t·∫Øt ng·∫Øn g·ªçn v·ªÅ ch·ªß ƒë·ªÅ.
   - S·∫Øp x·∫øp th√¥ng tin theo th·ª© t·ª± logic ho·∫∑c th·ªùi gian (n·∫øu ph√π h·ª£p).
   - S·ª≠ d·ª•ng c√°c ti√™u ƒë·ªÅ ph·ª• ƒë·ªÉ ph√¢n chia c√°c ph·∫ßn kh√°c nhau c·ªßa c√¢u tr·∫£ l·ªùi.

3. Ng√¥n ng·ªØ v√† phong c√°ch: 
   - S·ª≠ d·ª•ng ng√¥n ng·ªØ c·ªßa ng∆∞·ªùi d√πng trong to√†n b·ªô c√¢u tr·∫£ l·ªùi.
   - Duy tr√¨ phong c√°ch chuy√™n nghi·ªáp, kh√°ch quan v√† d·ªÖ hi·ªÉu.
   - Gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ chuy√™n ng√†nh v√† t√™n ri√™ng trong ng√¥n ng·ªØ g·ªëc.

4. X·ª≠ l√Ω th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß ho·∫∑c kh√¥ng ch·∫Øc ch·∫Øn:
   - N·∫øu th√¥ng tin kh√¥ng ƒë·∫ßy ƒë·ªß ho·∫∑c m√¢u thu·∫´n, h√£y n√™u r√µ ƒëi·ªÅu n√†y.
   - ƒê·ªÅ xu·∫•t c√°c h∆∞·ªõng t√¨m ki·∫øm ho·∫∑c ngu·ªìn b·ªï sung n·∫øu c·∫ßn thi·∫øt.

5. C·∫≠p nh·∫≠t v√† li√™n quan:
   - ∆Øu ti√™n th√¥ng tin m·ªõi nh·∫•t v√† li√™n quan nh·∫•t ƒë·∫øn truy v·∫•n.
   - N·∫øu c√≥ s·ª± kh√°c bi·ªát ƒë√°ng k·ªÉ gi·ªØa th√¥ng tin c≈© v√† m·ªõi, h√£y n√™u r√µ s·ª± thay ƒë·ªïi.

6. T∆∞∆°ng t√°c v√† theo d√µi:
   - K·∫øt th√∫c b·∫±ng c√°ch h·ªèi ng∆∞·ªùi d√πng xem h·ªç c·∫ßn l√†m r√µ ho·∫∑c b·ªï sung th√¥ng tin g√¨ kh√¥ng.
   - ƒê·ªÅ xu·∫•t c√°c c√¢u h·ªèi li√™n quan ho·∫∑c ch·ªß ƒë·ªÅ m·ªü r·ªông d·ª±a tr√™n n·ªôi dung t√¨m ki·∫øm.

N·ªôi dung t·ª´ c√°c trang web:
{web_contents}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n c√°c h∆∞·ªõng d·∫´n tr√™n v√† n·ªôi dung web ƒë∆∞·ª£c cung c·∫•p. ƒê·∫£m b·∫£o c√¢u tr·∫£ l·ªùi c·ªßa b·∫°n ch√≠nh x√°c v·ªõi th√¥ng tin t·ª´ c√°c trang web, to√†n di·ªán v√† h·ªØu √≠ch.
"""

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": default_system_prompt}]

if "user_system_prompt" not in st.session_state:
    st.session_state.user_system_prompt = ""

if "selected_models" not in st.session_state:
    st.session_state.selected_models = default_reference_models.copy()

if "conversations" not in st.session_state:
    st.session_state.conversations = []

if "conversation_deleted" not in st.session_state:
    st.session_state.conversation_deleted = False

if "show_modal" not in st.session_state:
    st.session_state.show_modal = False

if "edit_gpt_index" not in st.session_state:
    st.session_state.edit_gpt_index = None

if "selected_translation_model" not in st.session_state:
    st.session_state.selected_translation_model = "gemma2-9b-it"

if "web_search_enabled" not in st.session_state:
    st.session_state.web_search_enabled = False

# Set page configuration
st.set_page_config(page_title="Groq MoA Chatbot", page_icon="ü§ñ", layout="wide")

# Custom CSS
st.markdown(
    """
    <style>
    .sidebar-content {
        padding: 1rem;
    }
    .sidebar-content .custom-gpt {
        display: flex;
        align-items: center;
        justify-content: space-between;
        padding: 0.5rem;
        border-bottom: 1px solid #ccc.
    }
    .sidebar-content .custom-gpt:last-child {
        border-bottom: none.
    }
    .remove-button {
        background-color: transparent.
        color: red.
        border: none.
        cursor: pointer.
        font-size: 16px.
    }
    .modal {
        display: none.
        position: fixed.
        z-index: 1.
        left: 0.
        top: 0.
        width: 100%.
        height: 100%.
        overflow: auto.
        background-color: rgb(0,0,0).
        background-color: rgba(0,0,0,0.4).
        padding-top: 60px.
    }
    .modal-content {
        background-color: #fefefe.
        margin: 5% auto.
        padding: 20px.
        border: 1px solid #888.
        width: 80%.
    }
    .close {
        color: #aaa.
        float: right.
        font-size: 28px.
        font-weight: bold.
    }
    .close:hover,
    .close:focus {
        color: black.
        text-decoration: none.
        cursor: pointer.
    }
    </style>
    """,
    unsafe_allow_html=True
)

# Welcome message
welcome_message = """
# MoA (Mixture-of-Agents) Chatbot

Made by V√µ Mai Th·∫ø Long üë®‚Äçüè´

Powered by LLM models from Groq.com
"""

def process_fn(item, temperature=0.7, max_tokens=2048):
    references = item.get("references", [])
    model = item["model"]
    messages = item["instruction"]

    output = generate_with_references(
        model=model,
        messages=messages,
        references=references,
        temperature=temperature,
        max_tokens=max_tokens,
    )
    if DEBUG:
        logger.info(
            f"model {model}, instruction {item['instruction']}, output {output[:20]}",
        )

    st.write(f"Finished querying {model}.")

    return {"output": output}

def run_timer(stop_event, elapsed_time):
    start_time = time.time()
    while not stop_event.is_set():
        elapsed_time.set(time.time() - start_time)
        time.sleep(0.1)

def translate_response(response, translation_model, language_code):
    translated_response = translate_text(response, translation_model)
    return translated_response

def extract_url_from_prompt(prompt):
    # Implement a function to extract URL from the prompt
    import re
    url_pattern = re.compile(r'https?://\S+')
    url = url_pattern.search(prompt)
    return url.group(0) if url else None

def generate_search_query(conversation_history, current_query, language):
    # S·ª≠ d·ª•ng model Gemma-2-9B-IT ƒë·ªÉ t·∫°o query t√¨m ki·∫øm
    model = "gemma2-9b-it"
    
    # T·∫°o prompt cho model
    system_prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp trong vi·ªác t·∫°o query t√¨m ki·∫øm. 
    Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán v√† c√¢u h·ªèi hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng, 
    sau ƒë√≥ t·∫°o ra m·ªôt query t√¨m ki·∫øm ng·∫Øn g·ªçn, ch√≠nh x√°c v√† hi·ªáu qu·∫£. 
    Query n√†y s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin tr√™n web.
    H√£y ƒë·∫£m b·∫£o query bao g·ªìm c√°c t·ª´ kh√≥a quan tr·ªçng v√† b·ªëi c·∫£nh c·∫ßn thi·∫øt.
    T·∫°o query b·∫±ng ng√¥n ng·ªØ c·ªßa c√¢u h·ªèi ng∆∞·ªùi d√πng: {language}."""  # Th√™m h∆∞·ªõng d·∫´n ƒë·ªÉ t·∫°o query b·∫±ng ng√¥n ng·ªØ c·ªßa ng∆∞·ªùi d√πng

    user_prompt = f"""L·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán:
    {conversation_history}
    
    C√¢u h·ªèi hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng:
    {current_query}
    
    H√£y t·∫°o m·ªôt query t√¨m ki·∫øm ng·∫Øn g·ªçn v√† hi·ªáu qu·∫£ d·ª±a tr√™n th√¥ng tin tr√™n."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # G·ªçi API ƒë·ªÉ generate query
    generated_query = generate_together(
        model=model,
        messages=messages,
        max_tokens=100,
        temperature=0.7
    )

    return generated_query.strip()

def main():
    # Display welcome message
    st.markdown(welcome_message)

    # Sidebar for configuration
    with st.sidebar:

        # Custom border for Web Search and Additional System Instructions
        st.markdown('<div class="custom-border">', unsafe_allow_html=True)
        
        st.header("Web Search")
        web_search_enabled = st.checkbox("Enable Web Search", value=st.session_state.web_search_enabled)
        if web_search_enabled != st.session_state.web_search_enabled:
            st.session_state.web_search_enabled = web_search_enabled
            if web_search_enabled:
                st.session_state.selected_models = default_reference_models.copy()
                st.session_state.selected_models.append("llama3-groq-70b-8192-tool-use-preview")  # Th√™m m√¥ h√¨nh llama3-8b-8192

        st.header("Additional System Instructions")
        user_prompt = st.text_area("Add your instructions", value=st.session_state.user_system_prompt, height=100)
        if st.button("Update System Instructions"):
            st.session_state.user_system_prompt = user_prompt
            combined_prompt = f"{default_system_prompt}\n\nAdditional instructions: {user_prompt}"
            if len(st.session_state.messages) > 0:
                st.session_state.messages[0]["content"] = combined_prompt
            st.success("System instructions updated successfully!")
        
        st.markdown('</div>', unsafe_allow_html=True)  # Close the custom border

        st.header("Model Settings")
        
        with st.expander("Configuration", expanded=False):
            model = st.selectbox(
                "Main model (aggregator model)",
                default_reference_models,
                index=default_reference_models.index("llama3-groq-70b-8192-tool-use-preview") if st.session_state.web_search_enabled else 0
            )
            temperature = st.slider("Temperature", 0.0, 2.0, 0.5, 0.1)
            max_tokens = st.slider("Max tokens", 1, 8192, 2048, 1)

            st.subheader("Reference Models")
            for i, ref_model in enumerate(default_reference_models):
                if st.checkbox(ref_model, value=(ref_model in st.session_state.selected_models)):
                    if ref_model not in st.session_state.selected_models:
                        st.session_state.selected_models.append(ref_model)
                else:
                    if ref_model in st.session_state.selected_models:
                        st.session_state.selected_models.remove(ref_model)

            st.subheader("Translation Model")
            selected_translation_model = st.selectbox("Select Translation Model", default_reference_models, index=2)
            st.session_state.selected_translation_model = selected_translation_model

        # Start new conversation button
        if st.button("Start New Conversation", key="new_conversation"):
            st.session_state.messages = [{"role": "system", "content": st.session_state.messages[0]["content"]}]
            st.rerun()

        # Previous conversations
        st.subheader("Previous Conversations")
        for idx, conv in enumerate(reversed(st.session_state.conversations)):  # Reverse the list
            cols = st.columns([0.9, 0.1])
            with cols[0]:
                if st.button(f"{len(st.session_state.conversations) - idx}. {conv['first_question'][:30]}...", key=f"conv_{idx}"):
                    st.session_state.messages = conv['messages']
                    st.rerun()
            with cols[1]:
                if st.button("‚ùå", key=f"del_{idx}", on_click=lambda i=idx: delete_conversation(len(st.session_state.conversations) - i - 1)):
                    st.session_state.conversation_deleted = True

        # Add a download button for chat history
        if st.button("Download Chat History"):
            chat_history = "\n".join([f"{m['role']}: {m['content']}"] for m in st.session_state.messages[1:])  # Skip system message
            st.download_button(
                label="Download Chat History",
                data=chat_history,
                file_name="chat_history.txt",
                mime="text/plain"
            )

    # Trigger rerun if a conversation was deleted
    if st.session_state.conversation_deleted:
        st.session_state.conversation_deleted = False
        st.experimental_rerun()

    # Chat interface
    st.header("Hello! I am MoA chatbot, please send me your questions below.")
    
    # Display chat messages from history on app rerun
    for message in st.session_state.messages[1:]:  # Skip the system message
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # React to user input
    if prompt := st.chat_input("What would you like to know?"):
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # Detect language of the user's input
        user_language = detect(prompt)

        # Save first question of new conversation
        if len(st.session_state.messages) == 2:  # First user message
            st.session_state.conversations.append({
                "first_question": prompt,
                "messages": st.session_state.messages.copy()
            })

        # Generate response
        timer_placeholder = st.empty()
        stop_event = threading.Event()
        elapsed_time = SharedValue()
        timer_thread = threading.Thread(target=run_timer, args=(stop_event, elapsed_time))
        timer_thread.start()

        start_time = time.time()

        if st.session_state.web_search_enabled:
            try:
                st.session_state.messages[0]["content"] = web_search_prompt  # Update the system prompt for web search
                st.session_state.messages.append({"role": "assistant", "content": "ƒêang t√¨m ki·∫øm tr√™n web..."})


                with st.spinner("ƒêang t√¨m ki·∫øm tr√™n web..."):
                    # S·ª≠ d·ª•ng h√†m generate_search_query ƒë·ªÉ t·∫°o query
                    conversation_history = "\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state.messages[:-1]])
                    generated_query = generate_search_query(conversation_history, prompt, user_language)
                    
                    # Display the search query used
                    st.session_state.messages.append({"role": "system", "content": f"Search query: {generated_query}"})
                    st.chat_message("system").markdown(f"Search query: {generated_query}")

                    search_results = google_search(generated_query, num_results=10)  # Increase number of search results
                    
                    # Ki·ªÉm tra n·∫øu kh√¥ng c√≥ k·∫øt qu·∫£ t√¨m ki·∫øm
                    if 'items' not in search_results:
                        raise ValueError("No search results found.")
                    
                    snippets = extract_snippets(search_results)
                    sources = [item['link'] for item in search_results['items']]
                    
                    # Ghi log c√°c k·∫øt qu·∫£ t√¨m ki·∫øm v√† ƒë∆∞·ªùng link v√†o console
                    logger.info(f"Search snippets: {snippets}")
                    logger.info(f"Search sources: {sources}")
                    
                    search_summary = "\n\n".join(snippets)
                    # Kh√¥ng th√™m search_summary v√†o st.session_state.messages ƒë·ªÉ tr√°nh hi·ªÉn th·ªã tr√™n UI

                    # Use the search summary to generate a final response using the main model
                    data = {
                        "instruction": [st.session_state.messages] * len(st.session_state.selected_models),
                        "references": [[search_summary]] * len(st.session_state.selected_models),
                        "model": st.session_state.selected_models,
                    }
                    eval_set = datasets.Dataset.from_dict(data)

                    eval_set = eval_set.map(
                        partial(
                            process_fn,
                            temperature=temperature,
                            max_tokens=max_tokens,
                        ),
                        batched=False,
                        num_proc=len(st.session_state.selected_models),
                    )
                    references = [item["output"] for item in eval_set]
                    data["references"] = references
                    eval_set = datasets.Dataset.from_dict(data)

                    output = generate_with_references(
                        model=model,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        messages=st.session_state.messages,
                        references=references,
                        generate_fn=generate_together
                    )

                    full_response = ""
                    for chunk in output:
                        if isinstance(chunk, dict) and "choices" in chunk:
                            for choice in chunk["choices"]:
                                if "delta" in choice and "content" in choice["delta"]:
                                    full_response += choice["delta"]["content"]
                        else:
                            full_response += chunk

                    # Translate the response if necessary
                    if user_language != 'en':  # Assuming 'en' is the default language of the response
                        full_response = translate_text(full_response, st.session_state.selected_translation_model)

                    # Display the translated response with sources
                    # formatted_response = format_response_with_sources(full_response, sources)
                    formatted_response = full_response

                    with st.chat_message("assistant"):
                        st.markdown(formatted_response)
                    
                    st.session_state.messages.append({"role": "assistant", "content": formatted_response})
                    st.session_state.conversations[-1]['messages'] = st.session_state.messages.copy()

            except Exception as e:
                logger.error(f"Error during web search: {str(e)}")
                st.session_state.messages.append({"role": "assistant", "content": f"L·ªói khi t√¨m ki·∫øm tr√™n web: {str(e)}"})

        else:
            # Log main model and translation model
            logger.info(f"Main model: {model}")
            logger.info(f"Translation model: {st.session_state.selected_translation_model}")

            # Update model selection logic
            selected_models = list(set(st.session_state.selected_models))
            if model not in selected_models:
                selected_models.append(model)  # Ensure main model is included

            # Log selected models
            logger.info(f"Selected models: {selected_models}")

            data = {
                "instruction": [st.session_state.messages for _ in range(len(selected_models))],
                "references": [[] for _ in range(len(selected_models))],
                "model": selected_models,
            }

            eval_set = datasets.Dataset.from_dict(data)

            try:
                with st.spinner("Typing..."):
                    for i_round in range(1):
                        eval_set = eval_set.map(
                            partial(
                                process_fn,
                                temperature=temperature,
                                max_tokens=max_tokens,
                            ),
                            batched=False,
                            num_proc=len(selected_models),
                        )
                        references = [item["output"] for item in eval_set]
                        data["references"] = references
                        eval_set = datasets.Dataset.from_dict(data)
                        # Update timer display
                        timer_placeholder.markdown(f"‚è≥ **Elapsed time: {elapsed_time.get():.2f} seconds**")

                    output = generate_with_references(
                        model=model,
                        temperature=temperature,
                        max_tokens=max_tokens,
                        messages=st.session_state.messages,
                        references=references,
                        generate_fn=generate_together
                    )

                    full_response = ""
                    for chunk in output:
                        if isinstance(chunk, dict) and "choices" in chunk:
                            for choice in chunk["choices"]:
                                if "delta" in choice and "content" in choice["delta"]:
                                    full_response += choice["delta"]["content"]
                        else:
                            full_response += chunk

                    # Translate the response if necessary
                    if user_language != 'en':  # Assuming 'en' is the default language of the response
                        full_response = translate_text(full_response, st.session_state.selected_translation_model)

                    # Display the translated response
                    with st.chat_message("assistant"):
                        st.markdown(full_response)
                    
                    st.session_state.messages.append({"role": "assistant", "content": full_response})
                    st.session_state.conversations[-1]['messages'] = st.session_state.messages.copy()

                end_time = time.time()
                duration = end_time - start_time
                timer_placeholder.markdown(f"‚è≥ **Total elapsed time: {duration:.2f} seconds**")
                logger.info(f"Response generated in {duration:.2f} seconds")

            except Exception as e:
                st.error(f"An error occurred during the generation process: {str(e)}")
                logger.error(f"Generation error: {str(e)}")
            finally:
                stop_event.set()
                timer_thread.join()

def format_response_with_sources(response, sources):
    # Ghi log c√°c ngu·ªìn v√†o console
    logger.info(f"Sources: {sources}")
    # Kh√¥ng c·∫ßn th√™m c√°c ƒë∆∞·ªùng link v√†o ph·∫£n h·ªìi
    return response


if __name__ == "__main__":
    main()
